input {
	tcp {
		port => 5001
	}
}

## Add your filters / logstash plugins configuration here
filter {
    csv {
        columns => ["SrNo","KPI_ID","KpiName","from_dt","to_dt","region","region_type","Age","Age_Group","Gender","Sector","Disaster_type","Time_of_Occurence","Infrastructural_component","Refugee_camps","Crisis_Type","Nationality","UploadId","UID","UpdateDateOn","Data_Value1","Data_Value2","Quater","Period","Type_Equipment","Type_Road","Type_Loss","Status"]
		separator => ";"
		autodetect_column_names => true
		skip_empty_columns => true
		#convert => {
        #  "Data_Value1" => "integer"
        #  "Data_Value2" => "integer"
        #}
	}

	date {
        match => [ "from_dt","dd.MM.yy"]
		target => ["time_from"]
    }

	mutate {
		convert => ["Data_Value1","integer"]	
		convert => ["Data_Value2","integer"]
		remove_field => ["message", "from_dt"]
	}
	
}
output {
	stdout {}
	elasticsearch {
		hosts => "elasticsearch:9200"
		user => "elastic"
		password => "changeme"
	}
}
